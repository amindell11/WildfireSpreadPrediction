# ğŸ”¥ Wildfire Propagation Modeling with CNNs and Transformers

This project explores deep learning approaches for predicting wildfire spread across geospatial regions using satellite data. The primary goal was to compare the performance of a **Convolutional Autoencoder** and a **SegFormer transformer model** to determine whether transformer-based architectures are better suited for capturing complex spatial patterns in wildfire propagation.

All experiments were logged and visualized using [Weights & Biases (WandB)](https://wandb.ai), and the models were trained using NVIDIA A100 GPUs on Lambda Labs cloud instances.

---

## ğŸ“Œ Project Links

- **SegFormer Performance Dashboard (Organized)**: [wandb.ai/WildfirePropagation23-24v2](https://wandb.ai/arthur-v-qin/WildfirePropagation23-24v2?nw=nwuserarthurvqin)
- **Feature Ablation Study Dashboard**: [wandb.ai/WildfireAblation](https://wandb.ai/arthur-v-qin/WildfireAblation?nw=nwuserarthurvqin)
- **Autoencoder + Transformer Experiments Dashboard (Messy)**: [wandb.ai/WildfirePropagation23-24](https://wandb.ai/arthur-v-qin/WildfirePropagation23-24?nw=nwuserarthurvqin)

---

## ğŸ§  Models Compared

### ğŸŒ€ 1. Convolutional Autoencoder (CAE)
- Designed using skip connections and ResBlocks.
- Architecture inspired by [Huot et al. (2021)](https://arxiv.org/abs/2112.02447) (*'Next Day Wildfire Spread: A Machine Learning Data Set to Predict Wildfire Spreading from Remote-Sensing Data
'*) â€” their model diagram was used as a reference to reconstruct the CAE from scratch.
- Trained using different combinations of:
  - Learning rate
  - Batch size
  - Dropout rate
  - Epoch count

### ğŸ”² 2. SegFormer (Transformer-based Semantic Segmentation)
- Based on the SegFormer architecture introduced by NVIDIA.
- Implemented via HuggingFace's `TFSegformerForSemanticSegmentation`.
- Custom training loop built from scratch.
- All transformer-related code and metric handling was original (except data ingestion).
- Includes:
  - Variable encoder depth and hidden size
  - Custom metric evaluation function using `mean_iou`

---

## ğŸ§ª Feature Ablation

Ablation experiments were performed by removing individual input features (e.g. `tmmx`, `pdsi`, `NDVI`, etc.) to assess their impact on model performance. Each experiment logs performance to the WandB dashboard for side-by-side comparison.

See: [wandb.ai/WildfireAblation](https://wandb.ai/arthur-v-qin/WildfireAblation?nw=nwuserarthurvqin)

---

## ğŸ§¾ Paper Used for Reference

The model design and data handling pipeline were based heavily on:

> **Huot, F., Hu, L.R., Goyal, Nita., Sankar, T., Ihme, M., & Chen, Y. (2021).**  
> *Next Day Wildfire Spread: A Machine Learning Data Set to Predict Wildfire Spreading from Remote-Sensing Data*.  
> [arXiv:2112.02447](https://arxiv.org/abs/2112.02447)

While many utility functions and the data pipeline structure (including TFRecord parsing, input/output handling, and clipping) were adapted from this work, all model implementations â€” especially the full transformer training pipeline and custom CAE code â€” were written independently.

---

## ğŸ§° Setup Instructions

The following commands reproduce the cloud training environment used on Lambda Labs (Ubuntu + NVIDIA A100). You may need a Lambda Key to rent the GPUs. 

### ğŸ“¦ Clone the Repository
```bash
git clone https://github.com/Galagalagalaga/WildfireResearch23.git
```

### ğŸ“ Download Dataset
```bash
wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ..." "https://storage.googleapis.com/kaggle-data-sets/.../archive.zip" -c -O 'archive.zip'
```

### ğŸ“‚ Unzip Archive
```bash
unzip archive.zip
```

### ğŸ Set Up Python Environment with Miniconda
```bash
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh

~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh

conda create -n wildenv python=3.11.4
conda activate wildenv

pip install tensorflow wandb transformers evaluate matplotlib scikit-learn
```

### ğŸ§ª Run Experiments
```bash
cd WildfireResearch23/code
python file.py
```

---

## ğŸ“‚ Code Structure

```
WildfireResearch23/
â”œâ”€â”€ code/                                       # Code directory
â”‚   â”œâ”€â”€ .idea/                                  # ToySeg file dump, not important
â”‚       â”œâ”€â”€ ...                                 # Contents condensed
â”‚   â”œâ”€â”€ AUC PR/                                 # Code for custom metric which ended up not working with SegFormer
â”‚       â”œâ”€â”€ ...                                 # Contents condensed
â”‚   â”œâ”€â”€ models/                                 # Autogenerated folder meant to store model versions, but ended up using WandB instead
â”‚       â”œâ”€â”€ ...                                 # Contents condensed
â”‚   â”œâ”€â”€ wandb/                                  # Autogenerated folder storing wandb runs, but obsolete (used cloud storage instead)
â”‚       â”œâ”€â”€ ...                                 # Contents condensed
â”‚   â”œâ”€â”€ autoencoder.py                          # Convolutional Autoencoder
â”‚   â”œâ”€â”€ segformer.py                            # SegFormer Transformer
â”‚   â”œâ”€â”€ ablation.py                             # Feature ablation loop
â”‚   â”œâ”€â”€ constants.py                            # Feature names, dataset paths, and normalization stats
â”‚   â”œâ”€â”€ parameters.py                           # Hyperparameter grid search settings
â”‚   â”œâ”€â”€ utils.py                                # Data preprocessing, loading, visualization
â”‚   â”œâ”€â”€ ...                                     # Remaining contents condensed
â”œâ”€â”€ lambdakey/                                  # Directory storing LambdaLabs Key
â”‚   â”œâ”€â”€ testingkey.pem                          # My key (probably doesn't work anymore)
â”œâ”€â”€ notebooks/                                  # Testing scripts developed in early stages of project
â”‚   â”œâ”€â”€ toyseg/                                 # Folder for developing 'toy' versions of segformer segmentation
â”‚       â”œâ”€â”€ wandb/                              # Autogenerated folder storing wandb runs and models (irrelevant)
â”‚           â”œâ”€â”€ ...                             # Contents Condensed
â”‚       â”œâ”€â”€ SimplePytorchWandBTest.ipynb        # Weights and Biases testing script
â”‚       â”œâ”€â”€ ToySegmentation.ipynb               # SegFormer testing script
â”‚       â”œâ”€â”€ requirements.txt                    # Requirements file for easy lib imports
â”‚   â”œâ”€â”€ Wildfire23-24.ipynb                     # Science Fair notebook for Convolutional Autoencoder w/ extra visualizations at the end
â”‚   â”œâ”€â”€ Wildfire22-23.ipynb                     # Earlier iteration of the same project, with much simpler autoencoder model
â”‚   â”œâ”€â”€ ToySegmentation.py                      # Same file as within toyseg folder, just converted to python file


```

---

## ğŸ“ˆ Metrics + Evaluation

- Main evaluation metric: `mean_iou`
- `BinaryIoU` also used in CAE training
- While the reference paper used AUC-PR, implementation complexities led to using `mean_iou` as a consistent baseline across models

---

## ğŸ’¬ Notes

- All training/validation/test sets were built using filtered TFRecords with the same format as the reference paper.
- The CAE was entirely built from scratch using only a visual diagram.
- Transformer models required reshaping tensors and working around HuggingFace's defaults to handle custom input channels and spatial formats.
- Custom visualization tools were added in `utils.py` for displaying prediction masks and input features.

---

## ğŸ“œ License / Citation

You may use or build upon this code, but please cite the original paper [Huot et al. (2021)](https://arxiv.org/abs/2112.02447) if you make use of the dataset parsing utilities or data normalization strategies.

---
